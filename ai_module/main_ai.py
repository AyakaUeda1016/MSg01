# -*- coding: utf-8 -*-
"""
éŸ³å£°å…¥åŠ› â†’ Whisperæ–‡å­—èµ·ã“ã— â†’ GPTå¿œç­”
å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
    pip install openai faster-whisper opensmile sounddevice soundfile numpy
"""

from openai import OpenAI
from datetime import datetime
from pathlib import Path
import json, os, tempfile, time, gc, random
import numpy as np
import sounddevice as sd
import soundfile as sf
import opensmile
from faster_whisper import WhisperModel


# ==== ã‚·ãƒŠãƒªã‚ªè¨­å®š ====
SCENARIO = {
    "scene": "æœç¤¼å¾Œã®å‹é”ã¨ã®ä¼šè©±",
    "start_message": "ä»Šæ—¥ã®æˆæ¥­ã‚ã‚“ã©ãã•ã„ã­"
}

# ==== OpenAIè¨­å®š ====
client = OpenAI(api_key="ã‚¿ã‚±ãƒãƒ³ã‚»ãƒ³ã®APIã‚­ãƒ¼")  # â† ã‚ãªãŸã®APIã‚­ãƒ¼ã‚’è¨­å®š



# ==== Whisper / openSMILE ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ====
WHISPER = None
SMILE = None


def init_models():
    """Whisperã¨openSMILEã‚’åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¼ï¼‰"""
    global WHISPER, SMILE
    if WHISPER is None:
        print("[INIT] WhisperModel(small) ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        WHISPER = WhisperModel("small", device="cpu", compute_type="int8")
    if SMILE is None:
        print("[INIT] openSMILE(eGeMAPSv02) ã‚’åˆæœŸåŒ–ä¸­...")
        SMILE = opensmile.Smile(
            feature_set=opensmile.FeatureSet.eGeMAPSv02,
            feature_level=opensmile.FeatureLevel.Functionals,
        )


# ==== éŒ²éŸ³é–¢é€£ ====
def _dbfs(x: np.ndarray) -> float:
    """RMSã‹ã‚‰dBFSã‚’ç®—å‡º"""
    if x.dtype == np.int16:
        x = x.astype(np.float32) / 32768.0
    rms = np.sqrt(np.mean(x**2) + 1e-12)
    return 20.0 * np.log10(rms + 1e-12)


def record_until_silence(
    samplerate: int = 16000,
    frame_ms: int = 30,
    start_threshold_dbfs: float = -45.0,
    stop_silence_sec: float = 0.8,
    max_duration_sec: float = 30.0,
    warmup_sec: float = 0.3,
) -> np.ndarray:
    """è©±ã—å§‹ã‚ã‚’æ¤œçŸ¥â†’ç„¡éŸ³ã§åœæ­¢"""
    print("ğŸ¤ æº–å‚™ä¸­... 3ç§’ä»¥å†…ã«è©±ã™æº–å‚™ã‚’ã—ã¦ãã ã•ã„ã€‚")
    time.sleep(1)
    print("ğŸ‘‰ æº–å‚™OKï¼è©±ã—å§‹ã‚ã¦ãã ã•ã„ã€‚")

    buffers, started = [], False
    silence_run, total_time = 0.0, 0.0
    frame_samples = int(samplerate * frame_ms / 1000.0)

    def callback(indata, frames, time_info, status):
        nonlocal started, silence_run, total_time
        mono = indata[:, 0]
        level = _dbfs(mono)
        bar = "â–®" * max(0, min(30, int((level + 60) / 2)))
        print(f"\rğŸ“ˆ {level:6.1f} dBFS {bar:30}", end="", flush=True)

        if total_time >= warmup_sec:
            if not started and level >= start_threshold_dbfs:
                started = True
                print("\nğŸ™ï¸ éŒ²éŸ³é–‹å§‹ï¼")
            elif started:
                silence_run = silence_run + frame_ms / 1000.0 if level < start_threshold_dbfs else 0.0
        if started:
            buffers.append(mono.copy())
        total_time += frames / samplerate

    with sd.InputStream(
        samplerate=samplerate, channels=1, dtype="float32",
        blocksize=frame_samples, callback=callback
    ):
        start_ts = time.time()
        while True:
            time.sleep(frame_ms / 1000.0)
            if started and silence_run >= stop_silence_sec:
                print("\nâ¹ï¸ ç„¡éŸ³æ¤œå‡º â†’ åœæ­¢")
                break
            if time.time() - start_ts >= max_duration_sec:
                print("\nâ¹ï¸ æœ€å¤§éŒ²éŸ³æ™‚é–“ã«é”ã—ã¾ã—ãŸ")
                break

    if not buffers:
        print("[WARN] ç™ºè©±ãŒæ¤œçŸ¥ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return np.zeros((int(samplerate * 0.1),), dtype=np.float32)

    print(f"âœ… éŒ²éŸ³å®Œäº†ï¼ˆ{len(buffers)*frame_ms/1000:.1f}sï¼‰")
    return np.concatenate(buffers).astype(np.float32)


# ==== Whisperæ–‡å­—èµ·ã“ã— ====
def transcribe_whisper(audio: np.ndarray, samplerate: int = 16000):
    global WHISPER
    if WHISPER is None:
        init_models()

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        sf.write(tmp.name, audio, samplerate, format="WAV", subtype="PCM_16")
        tmp_path = tmp.name
    try:
        segments, info = WHISPER.transcribe(
            tmp_path, language="ja", task="transcribe",
            vad_filter=True, vad_parameters=dict(min_silence_duration_ms=700)
        )
        text = "".join([seg.text for seg in segments]).strip()
    finally:
        os.remove(tmp_path)
    if not text:
        return None
    return text


# ==== GPTåˆ¤å®šãƒ»å¿œç­” ====
CHARACTER_ROLE = "åŒç´šç”Ÿã®å‹é”"

def check_appropriateness(message, context, scene, start_message):
    """ç™ºè¨€ãŒä¼šè©±ã®æµã‚Œã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã€ç„¡é–¢ä¿‚ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    prompt = f"""
ã‚·ãƒ¼ãƒ³: {scene}
å°å…¥ä¼šè©±: {start_message}

ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´:
{context}

ç¾åœ¨ã®ç™ºè¨€:
{message}

ã“ã®ç™ºè¨€ã¯ã“ã‚Œã¾ã§ã®ä¼šè©±ã®æµã‚Œã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã€ãã‚Œã¨ã‚‚ç„¡é–¢ä¿‚ãªç™ºè¨€ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
ã€Œé–¢é€£ã™ã‚‹ç™ºè¨€ã€ã¾ãŸã¯ã€Œç„¡é–¢ä¿‚ãªç™ºè¨€ã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚
"""
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content.strip()


def generate_reply(message, context):
    """GPTè‡ªç„¶å¿œç­”"""
    prompt = f"""
ã‚ãªãŸã¯{CHARACTER_ROLE}ã§ã™ã€‚è½ã¡ç€ã„ãŸå„ªã—ã„ãƒˆãƒ¼ãƒ³ã§ã€1ã€œ2æ–‡ä»¥å†…ã§è¿”ç­”ã—ã¦ãã ã•ã„ã€‚
å±¥æ­´:
{context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€: {message}
"""
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150,
    )
    return res.choices[0].message.content.strip()


# ==== ãƒ¡ã‚¤ãƒ³ ====
def main():
    init_models()
    log_dir = Path("logs"); log_dir.mkdir(exist_ok=True)
    log_path = log_dir / f"conversation_gpt_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    history = [f"ã‚·ãƒ¼ãƒ³: {SCENARIO['scene']}", f"å°å…¥ä¼šè©±: {SCENARIO['start_message']}"]
    inappropriate = 0
    turn = 0  


    print(f"ğŸ™ï¸ ä¼šè©±é–‹å§‹: {SCENARIO['scene']}")
    print(f"\nğŸ¤– AI: {SCENARIO['start_message']}")

    # æœ€å¤§5ãƒ©ãƒªãƒ¼ã¯while turn < nã§æ±ºã‚ã¦ãã ã•ã„^o^
    while turn < 5:
        print(f"\n=== ğŸ” Turn {turn+1} ===")
        audio = record_until_silence()
        transcript = transcribe_whisper(audio)
        if not transcript:
            continue

        print(f"\nğŸ§ã‚ãªãŸ: {transcript}")

        context = "\n".join(history[-30:])

      
        # ä¼šè©±ã®æµã‚Œã¨ã®é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        judgment = check_appropriateness(transcript, context, SCENARIO['scene'], SCENARIO['start_message'])

        if judgment == "ç„¡é–¢ä¿‚ãªç™ºè¨€":
            inappropriate += 1
            reply = "âš ï¸ ç„¡é–¢ä¿‚ãªç™ºè¨€ã§ã™ã€‚ã‚‚ã†ä¸€åº¦è¨€ã„ç›´ã—ã¦ãã ã•ã„ã€‚"
            print(f"ğŸ¤– AI: {reply}")
            if inappropriate >= 2: # ç„¡é–¢ä¿‚ã‚«ã‚¦ãƒ³ãƒˆã¯if inappropriate >= nã§æ±ºã‚ã¦ãã ã•ã„^o^
                print("ğŸš« ä¼šè©±çµ‚äº†: ç„¡é–¢ä¿‚ç™ºè¨€ãŒå¤šã™ãã¾ã™ã€‚")
                break
            continue

        # é–¢é€£ã—ã¦ã„ã‚‹å ´åˆã®ã¿ turn ã‚’é€²ã‚ã‚‹
        reply = generate_reply(transcript, context)
        print(f"ğŸ¤– AI: {reply}")
        turn += 1

        # å±¥æ­´ã¨ãƒ­ã‚°ä¿å­˜
        history += [f"ã‚ãªãŸ: {transcript}", f"AI: {reply}"]
        with open(log_path, "a", encoding="utf-8") as f:
            json.dump({
                "turn": turn,
                "timestamp": datetime.now().isoformat(),
                "transcript": transcript,
                "reply": reply,
                "appropriateness": judgment,
                "scene": SCENARIO['scene'],
                "start_message": SCENARIO['start_message']
            }, f, ensure_ascii=False)
            f.write("\n")

        del audio, transcript
        gc.collect()

    print(f"\nğŸ’¾ ãƒ­ã‚°ä¿å­˜: {log_path}")
    print("ğŸ¯ ä¼šè©±çµ‚äº†ã€‚")


